\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm,bm,enumitem,hyperref}

\title{Trajectory-Aware PAC-Bayes Bounds with High-Dimensional Stochastic Error Control}
\author{Author Name}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]

\begin{document}
\maketitle

\begin{abstract}
We present PAC-Bayes bounds for high-dimensional models using trajectory-averaged curvature estimates.  
Our contributions include:

(i) explicit Gaussian KL decomposition into top-subspace alignment, residual spectral complexity, and trajectory curvature,  
(ii) stochastic error control for trace and log-determinant approximations with explicit dependence on dimension $d$, trajectory length $T$, Hutchinson samples $m$, and operator norms,  
(iii) analysis of posterior covariance inverse stability, and  
(iv) discussion of practical guidance for the user-specified posterior scaling constant $c_Q$.  

\textbf{Limitations:} Results are restricted to smooth losses; the log-determinant concentration bound is heuristic and assumes independence of Hessians along the trajectory.
\end{abstract}

\section{Introduction}
Classical PAC-Bayes bounds often rely on spectral alignment or Laplace approximations.  
Our work provides:

\begin{itemize}[noitemsep]
    \item Fully defined KL decomposition terms (top-subspace, residual, trajectory curvature).
    \item Dimensionally consistent stochastic error control for high-dimensional Hutchinson trace/log-det estimates.
    \item Explicit treatment of trajectory-averaged Hessian stability.
    \item Practical discussion of posterior scaling $c_Q$.
\end{itemize}

\textbf{Restrictions:} Smooth losses only; independence assumptions on Hessians are heuristic and discussed in Section~\ref{sec:kl_stochastic}.

\section{PAC-Bayes Preliminaries}
Let $S = \{(x_i,y_i)\}_{i=1}^n \sim \mathcal{D}$ with bounded loss $\ell(w;(x,y)) \in [0,1]$.  
Define $R_S(w) = \frac{1}{n}\sum_i \ell(w;(x_i,y_i))$, $R(w) = \mathbb{E}[\ell(w;(x,y))]$.

\begin{theorem}[Conditional PAC-Bayes, Catoni 2007; Grünwald 2021]
Let $P(\cdot|U)$ be a prior constructed from independent side information $U$, and $Q$ any posterior.  
Then for $\delta\in(0,1)$:
\[
\mathbb{E}_{w\sim Q}[R(w)] \le R_S(Q) + \sqrt{\frac{\mathrm{KL}(Q\|P(\cdot|U)) + \log(2\sqrt{n}/\delta)}{2n}}.
\]
\end{theorem}

\section{Background and Related Work}

\paragraph{PAC-Bayes Bounds.}
PAC-Bayesian theory \cite{mcallester1999pac,catoni2007pac} provides high-probability bounds on the expected risk of a posterior $Q$ relative to a prior $P$.  
Conditional PAC-Bayes \cite{grunwald2021pac} allows the prior $P$ to depend on side information while maintaining validity.

\paragraph{Spectral and Curvature-Aware Posteriors.}
Gaussian priors with spectral structure \cite{dziugaite2017computing,dziugaite2018data} exploit low-rank covariance to reduce ambient-dimension dependence.  
Curvature-aware posteriors \cite{martens2014new,maclaurin2015gradient} encode local curvature from the Hessian.  
Our contribution introduces trajectory-averaged curvature along optimization paths.

\paragraph{Stochastic Trace and Log-Determinant Estimation.}
Hutchinson’s estimator \cite{hutchinson1990stochastic} provides unbiased trace estimation.  
Matrix concentration inequalities \cite{tropp2012user,avron2011randomized} allow high-dimensional error control for trace approximations.  
\textbf{Caveat:} Applying Hutchinson to $\log(I+\lambda^{-1}H_t)$ is heuristic; concentration relies on independence and Lipschitz arguments, which may not hold for correlated SGD steps.

\paragraph{High-Dimensional PAC-Bayes Analysis.}
Existing bounds \cite{dziugaite2017computing,neyshabur2017exploring} assume local quadratic losses or spectral alignment.  
We avoid these assumptions but note restrictions discussed in Section~\ref{sec:kl_stochastic}.

\section{Trajectory-Averaged Posterior and Curvature Terms}

\subsection{Definitions}
Let $\{\widehat{w}_t\}_{t=1}^T$ be model parameters along a trajectory (e.g., SGD).  
Let $H_t = \nabla^2 R_S(\widehat{w}_t)$ (or its unbiased stochastic estimator).

\begin{definition}[Trajectory-Averaged Hessian]
\[
\overline{H}_T := \frac{1}{T}\sum_{t=1}^T H_t.
\]
\end{definition}

\begin{assumption}[Hessian Boundedness]
\(\|H_t\|_{\mathrm{op}} \le L_H\) for all $t$ with probability 1.
\end{assumption}

\begin{assumption}[Inverse Stability]
\(\lambda_{\min}(\overline{H}_T + \lambda I) \ge \lambda_0 > 0\)
ensuring the posterior covariance
\(\Sigma_Q := c_Q^2 (\overline{H}_T + \lambda I)^{-1}\)
is well-conditioned.
\end{assumption}

\subsection{KL Decomposition}
Let $V_r$ denote the top-$r$ eigenvectors of $\Sigma_P$.  
Define:
\begin{align}
\text{Top-subspace alignment: } & A_r := \|\Pi_r \widehat{w}_T\|_{\Sigma_P^{-1}}^2, \quad \Pi_r := V_r V_r^\top \\
\text{Residual spectral complexity: } & R_r := \mathrm{tr}(\Sigma_P^{-1}\Sigma_Q) - \mathrm{tr}(\Pi_r \Sigma_P^{-1}\Sigma_Q \Pi_r) \\
\text{Trajectory curvature (stochastic log-det): } & \widehat{\mathrm{FD}}_T := \frac{1}{T} \sum_{t=1}^T \frac{1}{m} \sum_{i=1}^m z_{t,i}^\top \log(I+\lambda^{-1} H_t) z_{t,i}, \quad z_{t,i}\sim \mathcal{N}(0,I)
\end{align}
All terms are now mathematically defined.  
\textbf{Remark:} $\widehat{\mathrm{FD}}_T$ concentration is heuristic and assumes independence of $\{H_t\}$.

\section{Rigorous KL Decomposition and Stochastic Error Control}
\label{sec:kl_stochastic}

\subsection{Gaussian KL Decomposition}
Let $P = \mathcal{N}(\mu_P, \Sigma_P)$ and $Q = \mathcal{N}(\mu_Q, \Sigma_Q)$.  
The exact KL divergence is
\begin{equation}
\mathrm{KL}(Q\|P) = \frac{1}{2} \Big[ \mathrm{tr}(\Sigma_P^{-1} \Sigma_Q) + (\mu_P - \mu_Q)^\top \Sigma_P^{-1} (\mu_P - \mu_Q) - d + \log\frac{\det \Sigma_P}{\det \Sigma_Q} \Big].
\label{eq:exact_gaussian_kl}
\end{equation}

\paragraph{Top-$r$ Subspace and Residual Decomposition.}
Let $V_r \in \mathbb{R}^{d \times r}$ be the top-$r$ eigenvectors of $\Sigma_P$, and $\Pi_r := V_r V_r^\top$. Then
\begin{align}
\mathrm{tr}(\Sigma_P^{-1}\Sigma_Q) &= \mathrm{tr}(\Pi_r \Sigma_P^{-1}\Sigma_Q \Pi_r) + \mathrm{tr}((I-\Pi_r)\Sigma_P^{-1}\Sigma_Q(I-\Pi_r)), \\
(\mu_P-\mu_Q)^\top \Sigma_P^{-1} (\mu_P-\mu_Q) &= \| \Pi_r (\mu_P-\mu_Q) \|_{\Sigma_P^{-1}}^2 + \| (I-\Pi_r)(\mu_P-\mu_Q)\|_{\Sigma_P^{-1}}^2.
\end{align}
All terms are dimensionally consistent.

\subsection{Stochastic Trace Concentration}
\begin{lemma}[Residual Trace Concentration]
Under bounded Hessians and independence of $\{H_t\}$, with probability at least $1-\delta_{\mathrm{tr}}$,
\[
\Big| \mathrm{tr}(\Sigma_P^{-1} \Sigma_Q) - \mathbb{E}[\mathrm{tr}(\Sigma_P^{-1} \Sigma_Q)] \Big| 
\le L_H \sqrt{\frac{2 \log(2d/\delta_{\mathrm{tr}})}{T \lambda_0^2}} =: \epsilon_{\mathrm{tr}}.
\]
\textbf{Remark:} This assumes independence along the trajectory; in practice, correlations from SGD may violate the bound.
\end{lemma}

\subsection{Stochastic Log-Determinant Bounds}
\begin{lemma}[Hutchinson Log-Det Concentration, Heuristic]
For $z_{t,i} \sim \mathcal{N}(0,I)$ i.i.d., define
\[
\widehat{\mathrm{FD}}_t := \frac{1}{m} \sum_{i=1}^m z_{t,i}^\top \log(I + \lambda^{-1} H_t) z_{t,i}.
\]
With probability at least $1-\delta_{\mathrm{FD}}$,
\[
\Big| \mathrm{tr} \log(I + \lambda^{-1} H_t) - \widehat{\mathrm{FD}}_t \Big| \le \epsilon_t := \frac{L_H}{\lambda} \sqrt{\frac{2 \log(2d/\delta_{\mathrm{FD}})}{m}}.
\]
\textbf{Remark:} Concentration for $\log$ of a matrix is heuristic; a rigorous proof requires more sophisticated matrix function inequalities.
\end{lemma}

\subsection{Dimensionally Consistent PAC-Bayes Bound}
Define the total stochastic error:
\[
\epsilon := \epsilon_{\mathrm{tr}} + \frac{1}{T} \sum_{t=1}^T \epsilon_t.
\]

\begin{theorem}[Trajectory-Aware PAC-Bayes Bound for Smooth Losses]
For smooth bounded losses and $\delta \in (0,1)$, with probability at least $1-\delta$,
\begin{equation}
\mathbb{E}_{w\sim Q}[R(w)] \le R_S(Q) + \sqrt{\frac{ \mathrm{KL}(Q\|P) + \log(1/\delta) + \epsilon}{2n}},
\end{equation}
where $\mathrm{KL}(Q\|P)$ is the exact Gaussian KL in \eqref{eq:exact_gaussian_kl}.
\end{theorem}

\section{Discussion and Practical Guidance}
\begin{itemize}[noitemsep]
    \item \textbf{Scaling constant $c_Q$:} User-defined to balance KL and stochastic error; larger $c_Q$ increases posterior variance and KL term, reducing tightness but improving stability.
    \item \textbf{Top-$r$ projection:} Choose $r$ according to spectral decay; captures dominant directions for alignment.
    \item \textbf{Hutchinson samples $m$:} Controls trade-off between computation and stochastic log-det error.
    \item \textbf{Trajectory length $T$:} Averaging improves concentration but independence assumption is heuristic.
    \item \textbf{Bound tightness:} Dimensionally consistent but may be loose; separates top-subspace, residual, and stochastic log-det contributions implicitly.
    \item \textbf{Loss smoothness:} Bound applies only to smooth losses; ReLU/hinge require alternative curvature estimators.
\end{itemize}

\end{document}