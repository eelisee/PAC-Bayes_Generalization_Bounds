\section{Implementation Guide}
\label{sec:implementation}

\subsection{Codebase Overview}
The implementation is structured into modular components designed for reproducibility and extensibility.  
Each module corresponds directly to one conceptual part of the theoretical framework, allowing clear mapping from equations to code.  

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{/data:} utilities for loading labeled and unlabeled data, computing feature maps $\phi(x)$, and splitting datasets into $(S, U)$.
    \item \textbf{/models:} definitions of linear, kernelized, and neural models returning both predictions and differentiable loss functions.
    \item \textbf{/optimization:} SGD-based training routines producing $\widehat{w}$, with options for learning rate, batch size, and noise injection.
    \item \textbf{/geometry:} spectral prior computation (covariance estimation, eigen-decomposition) and Hessian or Fisher curvature estimation.
    \item \textbf{/bounds:} numerical evaluation of each PAC-Bayes term and final bound computation according to Eq.~(\ref{eq:bound_computation}).
    \item \textbf{/experiments:} driver scripts performing full experimental runs, logging, and visualization.
\end{itemize}

All numerical operations rely on \texttt{NumPy} or \texttt{PyTorch} tensors, using double precision for stability.  
Matrix decompositions are truncated to the top-$r$ eigenvalues to ensure efficiency for large models.

\subsection{Variable Definitions}
The main variables in the implementation correspond to the following mathematical objects:

\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{ll}
    Symbol & Description \\ \hline
    $x_i, y_i$ & Labeled training samples, $i=1,\dots,n$ \\
    $x'_j$ & Unlabeled samples, $j=1,\dots,m$ \\
    $\phi(x)$ & Feature mapping (identity for linear, Nystr√∂m embedding for kernel models) \\
    $\widehat{\Sigma}_U$ & Empirical feature covariance from unlabeled data (Eq.~\ref{eq:sigmaU}) \\
    $\Sigma_P$ & Spectral prior covariance (Eq.~\ref{eq:prior_cov}) \\
    $P=\mathcal{N}(0,\sigma_P^2\Sigma_P)$ & Prior distribution \\
    $H(\widehat{w})$ & Empirical Hessian of loss at optimum \\
    $Q=\mathcal{N}(\widehat{w},c_Q^2(H+\lambda I)^{-1})$ & Posterior distribution \\
    $d_{\mathrm{eff}}(\tau)$ & Effective spectral dimension (Eq.~\ref{eq:deff}) \\
    $\mathrm{FD}(\widehat{w},S)$ & Flatness descriptor (Eq.~\ref{eq:FD}) \\
    $E_{\text{Mahalanobis}}$ & Mahalanobis energy $\widehat{w}^\top \Sigma_P^{-1}\widehat{w}$ \\
    $\widehat{R}_{\text{bound}}$ & Final generalization bound (Eq.~\ref{eq:bound_computation}) \\
\end{tabular}
\end{center}

All scalar hyperparameters $(\sigma_P, c_Q, \lambda, \kappa, \delta)$ are defined globally in a configuration file.

\subsection{Core Algorithm}
Algorithm~\ref{alg:pacbayes} describes the full computation pipeline.  
Each block corresponds to a code module in the directory layout described above.

\begin{algorithm}[h!]
\caption{Geometry- and Curvature-Aware PAC-Bayes Bound Computation}
\label{alg:pacbayes}
\begin{algorithmic}[1]
\Require Training data $S=\{(x_i,y_i)\}_{i=1}^n$, unlabeled data $U=\{x'_j\}_{j=1}^m$, parameters $(\sigma_P, c_Q, \lambda, \kappa, \delta)$
\Statex

\State \textbf{Step 1: Spectral Prior Construction}
    \State Compute feature matrix $\Phi_U = [\phi(x'_1),\dots,\phi(x'_m)]^\top$
    \State Estimate covariance $\widehat{\Sigma}_U = \frac{1}{m}\Phi_U^\top \Phi_U$
    \State Compute eigenpairs $(V,\Lambda)$ of $\widehat{\Sigma}_U$
    \State Form spectral shrinkage $f_\kappa(\Lambda) = \frac{\Lambda}{\Lambda+\kappa I}$
    \State Set $\Sigma_P = V f_\kappa(\Lambda)V^\top$
    \State Define prior $P = \mathcal{N}(0, \sigma_P^2 \Sigma_P)$
\Statex

\State \textbf{Step 2: Model Training}
    \State Initialize model weights $w_0$
    \For{$t=1$ to $T$}
        \State Draw mini-batch $B_t \subset S$
        \State $w_t \gets w_{t-1} - \eta_t \nabla_{w} \frac{1}{|B_t|}\sum_{(x,y)\in B_t}\ell(w_{t-1};(x,y))$
    \EndFor
    \State Set $\widehat{w} = w_T$
\Statex

\State \textbf{Step 3: Posterior Definition}
    \State Approximate Hessian $H(\widehat{w}) = \nabla^2 R_S(\widehat{w})$
    \State Define posterior $Q = \mathcal{N}(\widehat{w}, c_Q^2 (H+\lambda I)^{-1})$
\Statex

\State \textbf{Step 4: Bound Term Computations}
    \State $E_{\text{Mahalanobis}} \gets \widehat{w}^\top \Sigma_P^{-1} \widehat{w}$
    \State $d_{\mathrm{eff}}(\tau) \gets \sum_{i=1}^{p} \frac{\lambda_i}{\lambda_i+\tau}$, with $\tau=\max(\lambda,\kappa)$
    \State $\mathrm{FD}(\widehat{w},S) \gets \log\det(I+\lambda^{-1} H(\widehat{w}))$
    \State Evaluate empirical risk $R_S(\widehat{w})$
\Statex

\State \textbf{Step 5: Final Bound Evaluation}
    \State $\widehat{R}_{\text{bound}} \gets R_S(\widehat{w}) 
    + \sqrt{\frac{1}{2n}\left(\frac{E_{\text{Mahalanobis}}}{\sigma_P^2}
    + C_1 d_{\mathrm{eff}}(\tau)
    + C_2 \mathrm{FD}(\widehat{w},S)
    + \log(1/\delta)\right)}$
\State \Return $\widehat{R}_{\text{bound}}$
\end{algorithmic}
\end{algorithm}

\subsection{Implementation Notes}

\paragraph{Spectral truncation.}
For large $p$, store only the top-$r$ eigenvectors of $\widehat{\Sigma}_U$, with $r\ll p$.  
In this case, all matrix products involving $\Sigma_P$ or $\Sigma_P^{-1}$ can be performed in the reduced subspace, giving $O(rp)$ cost.

\paragraph{Hessian approximation.}
For neural models or high-dimensional parameters, the exact Hessian is intractable.  
We approximate $H$ by the empirical Fisher information:
\[
H_F = \frac{1}{n}\sum_{i=1}^n \nabla_w \ell(w;(x_i,y_i))\nabla_w \ell(w;(x_i,y_i))^\top.
\]
The log-determinant in $\mathrm{FD}$ is then computed via stochastic trace estimation.

\paragraph{Numerical stability.}
Regularization parameters $\lambda$ and $\kappa$ prevent singular matrices.  
We recommend $\lambda\in[10^{-4},10^{-2}]$ and $\kappa\in[10^{-3},10^{-1}]$ depending on feature scaling.

\paragraph{Complexity analysis.}
Each training step is $O(bp)$ for batch size $b$, while prior computation is $O(mp^2)$ and Hessian-based flatness estimation is $O(Kp)$ for $K$ stochastic vectors.  
For moderate feature dimensions ($p\lesssim10^3$), the entire pipeline runs within seconds on standard hardware.

\paragraph{Reproducibility.}
All random seeds (data splits, initialization, SGD shuffling) should be fixed for each experiment.  
Intermediate results (covariance matrices, eigenvalues, Hessians) are saved in \texttt{/results/} for downstream analysis.

\subsection{Output and Logging}
Each run produces:
\begin{itemize}[nosep]
    \item Numerical values of $(E_{\text{Mahalanobis}}, d_{\mathrm{eff}}, \mathrm{FD}, \widehat{R}_{\text{bound}})$,
    \item Training and test losses,
    \item Optional spectral and curvature plots (e.g., eigenvalue spectra of $\widehat{\Sigma}_U$ and $H$),
    \item CSV summary file for reproducible analysis.
\end{itemize}

\subsection{Verification Checklist}
A successful implementation should satisfy:
\begin{enumerate}[nosep]
    \item \textbf{Sanity check:} for isotropic priors ($\Sigma_P=I$) and constant curvature ($H=I$), the bound reduces to the classical PAC-Bayes form.
    \item \textbf{Monotonicity check:} as learning rate or SGD noise increases, $\mathrm{FD}$ decreases and $\widehat{R}_{\text{bound}}$ tightens.
    \item \textbf{Spectral check:} for low-rank synthetic data, $d_{\mathrm{eff}}$ is much smaller than $p$, and the bound reflects this improvement.
\end{enumerate}

\subsection{Summary}
This implementation guide provides a direct and reproducible path from the theoretical framework to empirical validation.  
Each symbol and equation from the theoretical derivation has an explicit computational counterpart, allowing transparent verification of the proposed generalization bound and its dependence on data geometry and optimization curvature.